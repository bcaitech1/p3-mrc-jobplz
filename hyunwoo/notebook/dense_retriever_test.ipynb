{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131fc0f2-fa10-4405-ba05-a3a6a770d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    XLMRobertaModel, AdamW, TrainingArguments, \n",
    "    get_linear_schedule_with_warmup, RobertaModel\n",
    ")\n",
    "\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    TensorDataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba049ce2-6aed-43f4-b7c4-d0b1f437b3b5",
   "metadata": {},
   "source": [
    "# 데이터셋 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94bc3461-f278-4ed8-af9a-b7a3d1a80d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('/opt/ml/input/data/data/train_dataset')\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd0733-a4f3-4652-b679-4f8d66d1f337",
   "metadata": {},
   "source": [
    "# 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6634e02f-b212-4d29-a75e-d91e31741275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5c1528-f832-4781-9a72-a13cef11ed71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 23545, 9414, 10459, 14279, 20625, 23545, 9414, 14279, 113, 10609, 10859, 22294, 114, 9632, 9543, 14279, 17730, 12030, 23545, 9637, 56356, 9414, 73295, 11903, 119, 165, 182, 165, 182, 22458, 20479, 9365, 43022, 44220, 10739, 9414, 74125, 55635, 22143, 119, 8844, 9689, 21928, 123, 45441, 9414, 74125, 73295, 9428, 52363, 16855, 10407, 45441, 9414, 74125, 78686, 8908, 103764, 11506, 119, 9644, 46216, 127, 10954, 33542, 117, 92360, 101814, 10462, 21789, 16323, 9694, 122, 120, 124, 119108, 9414, 74125, 51684, 9415, 11261, 9428, 52363, 13374, 9568, 42337, 10530, 9356, 75884, 11903, 119, 165, 182, 165, 182, 22458, 20479, 9414, 84467, 23545, 9952, 14279, 11882, 11018, 9056, 78131, 23545, 70672, 10622, 9460, 30134, 11467, 23969, 23545, 9568, 42337, 9966, 16605, 52961, 8844, 22200, 9095, 78505, 23969, 8932, 20595, 11925, 119, 9952, 73295, 9435, 40032, 11882, 8885, 17730, 10530, 18154, 8917, 11102, 117, 70672, 10622, 104431, 9069, 11903, 57138, 8896, 32537, 51684, 9901, 14867, 14843, 8917, 11102, 10622, 102838, 13767, 8909, 36553, 10622, 9069, 37824, 12178, 8932, 20595, 12030, 9321, 14867, 9414, 84467, 37650, 9689, 11513, 9069, 37824, 14102, 119, 9701, 9794, 12692, 55530, 51533, 16323, 117, 9641, 12692, 28981, 10739, 16323, 38401, 9689, 9670, 14646, 12638, 9689, 9637, 88168, 9069, 37824, 12178, 8932, 20595, 11925, 119, 8924, 11261, 9640, 13374, 8910, 69725, 9901, 73380, 117, 8900, 38688, 10459, 9644, 16758, 10530, 18154, 9095, 10459, 117, 9597, 20479, 9678, 47289, 10530, 18154, 9484, 12030, 9121, 9487, 43962, 10622, 9599, 12178, 8917, 11102, 10892, 29414, 9414, 108280, 14153, 19105, 11506, 119, 23289, 9952, 108280, 18154, 8880, 17730, 9566, 14843, 113, 9952, 74125, 9341, 34951, 10622, 8863, 14646, 14843, 8917, 11102, 9121, 114, 9633, 9064, 103088, 11903, 119, 92360, 10459, 9644, 62675, 9640, 13374, 8929, 18623, 14801, 18392, 9460, 118964, 10530, 40364, 9952, 84467, 9706, 16439, 18622, 14153, 8929, 18623, 15387, 9341, 34951, 10622, 9248, 27023, 12310, 9471, 11903, 119, 9069, 37824, 15387, 9576, 11261, 8865, 47181, 30005, 86834, 8857, 119432, 24756, 9952, 73295, 23545, 9568, 42337, 9966, 16605, 52961, 14153, 9913, 92564, 73352, 9587, 59095, 113, 8896, 28000, 71439, 47181, 30005, 86834, 12310, 20595, 114, 9637, 9678, 50632, 10739, 13767, 9321, 14867, 9414, 74125, 28467, 9952, 14279, 34951, 10739, 9706, 16439, 18622, 14153, 9435, 40032, 10739, 47058, 9115, 82034, 89184, 9913, 92564, 73352, 9587, 59095, 9678, 50632, 10622, 9672, 78705, 12453, 9379, 30858, 12692, 71439, 47181, 30005, 86834, 12310, 20595, 43739, 9356, 86834, 14863, 49636, 9064, 21928, 35506, 71689, 9954, 24190, 119, 9638, 28467, 92383, 9414, 84467, 9952, 73295, 16439, 8996, 66540, 119254, 36240, 54480, 9388, 12508, 12310, 9469, 21614, 93222, 25258, 8909, 14863, 92383, 100, 9323, 24017, 12178, 9637, 56356, 9379, 16605, 14871, 15387, 9405, 83616, 11513, 9328, 12508, 12178, 8932, 20595, 11925, 119, 9414, 84467, 8929, 118963, 11102, 9744, 12692, 12945, 50632, 10459, 108462, 9519, 25503, 14867, 9341, 34951, 10622, 104304, 8996, 11018, 108462, 9113, 29364, 11664, 9952, 73295, 73306, 9341, 34951, 10622, 9460, 16605, 13374, 25805, 9952, 108280, 9098, 118794, 26737, 30005, 75884, 11903, 119, 34079, 9328, 72687, 9059, 14279, 54480, 9388, 12508, 12310, 9469, 21614, 9956, 40818, 9309, 12692, 9328, 12508, 12178, 24190, 119, 8985, 119235, 134, 10302, 118, 10983, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(dataset['train'][0]['context'], padding='max_length', truncation=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fc4976-cd8a-4813-b698-3c49b68b6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = np.random.choice(range(len(dataset['train'])), 128)\n",
    "# training_dataset = dataset['train'][sample_idx]\n",
    "# print(len(dataset['train']), len(training_dataset['title']))\n",
    "training_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b2e4e3-1c60-430e-8c3c-c8f68b15cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_seqs = tokenizer(training_dataset['question'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4096f6-449d-4318-b476-413277dba5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'],\n",
    "                             q_seqs['input_ids'], q_seqs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6146dbfc-a6ab-45b6-9f13-88619228ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(XLMRobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEncoder, self).__init__(config)\n",
    "        \n",
    "        self.roberta = XLMRobertaModel(config)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d810d8e-78c8-4388-93d0-2d86e363a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing RobertaEncoder: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaEncoder were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing RobertaEncoder: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaEncoder were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled\n"
     ]
    }
   ],
   "source": [
    "p_encoder = RobertaEncoder.from_pretrained(model_name)\n",
    "q_encoder = RobertaEncoder.from_pretrained(model_name)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()\n",
    "    print('GPU enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2956512c-b762-42bb-9808-8c0556f88ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, dataset, p_model, q_model):\n",
    "    \n",
    "    # Dataloader\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "    \n",
    "    # Optimizer\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay' : args.weight_decay},\n",
    "        {'params': [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay' : 0.0},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay' : args.weight_decay},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if  any(nd in n for nd in no_decay)], 'weight_decay' : 0.0},       \n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    p_model.zero_grad()\n",
    "    q_model.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_iterator = trange((int(args.num_train_epochs)), desc='Epoch')\n",
    "    \n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc='Iteration')\n",
    "        \n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            q_encoder.train()\n",
    "            p_encoder.train()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "                \n",
    "            p_inputs = {\n",
    "                'input_ids' : batch[0],\n",
    "                'attention_mask' : batch[1]\n",
    "            }\n",
    "            \n",
    "            q_inputs = {\n",
    "                'input_ids' : batch[2],\n",
    "                'attention_mask' : batch[3]\n",
    "            }\n",
    "            \n",
    "            p_outputs = p_model(**p_inputs)\n",
    "            q_outputs = q_model(**q_inputs)\n",
    "            \n",
    "            # Calculate Similarity\n",
    "            sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))\n",
    "            \n",
    "            # target\n",
    "            targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.to('cuda')\n",
    "            \n",
    "            sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "            \n",
    "            loss = F.nll_loss(sim_scores, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            q_model.zero_grad()\n",
    "            p_model.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            \n",
    "    return q_model, p_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70acbdd9-5a3b-4f04-8aac-ddba8b2f5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='dense_retrieval',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=40,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76817fdf-ecbf-4896-9ba5-8a7c0ed1a7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21935184995345b5b8a2ab3c92e710f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=494.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_encoder, q_encoder = train(args, train_dataset, p_encoder, q_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec81b8a5-315d-498f-9575-32c91949f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 남부 지방에서 급격하게 영국산 제품이 많이 팔린 계기가 된 조치는? 통상금지법(Embargo Act of 1807)은 1807년에 미국 의회에서 통과된 법률이며 , 나폴레옹 전쟁을 치루고 있는 영국과 프랑스에 대항하여 미국의 무역을 금지한 것이었다\\n\\n이 통상금지(이하, 엠바고)는 미국의 상인과 화물선이 유럽의 해군에게 금수품으로 나포하면서, 미국의 중립 위반에 대한 보복 조치를 취하게 되자 이 법안을 시행하게 되었다. 영국 해군은 특히 수천 명의 미국 선원들을 자신들의 함대에 복역하게 하는 강제 징발 조치를 감행하였다. 유럽에 대한 통제권을 위해 필사적으로 다투었던 영국과 프랑스는 우발적인 사고이고, 그들의 생존을 위해 필수적인 조치였다고 합리화했다. 미국인들은 체사피크 레오퍼드 사건을 중립을 지키는 미국의 조치에 대한 영국의 어처구니 없는 위반 사례라고 보았다 외교적 모욕이라고 이해를 하고, 유럽 열강에 의해 이러한 작전들을 지지하는 보증되지 않은 공식적인 명령서는 미국의 선전포고에 대한 근거로서 널리 인정되고 있다. 노한 군중들은 냉소적으로 ‘엠바고’(Embargo)라는 글자를 거꾸로 ‘나 좀 잡아줘’(O Grab Me)라고 바꿔 불렀다. \\n\\n토머스 제퍼슨 대통령은 이러한 분노한 대중들의 적의를 등에 업고, 보복에 대한 대중적 지지에 힘을 싣고 있었다. 그는 의회에 군사적으로 물리적 조치보다는 경제 전쟁으로 응전해야 한다고 권했다. 그리하여 통상금지법이 1807년 12월 22일 서명되었다. 이 급작스런 조치에 대한 기대 효과(교전국 사이의 경제적 곤란 )는 영국과 프랑스를 힘들게 하여, 그들로 하여금 미국 상선을 방해를 하지못하도록 강제하고, 미국의 중립을 존중하며, 강제 징발 정책을 끝내는 것이었다. 이 엠바고는 강압적인 조치로서는 그다지 효과를 보지 못한 것으로 밝혀졌으며, 외교적으로도, 경제적으로 모두 실패한 조치였다. 이 법이 막상 시행되고 나서, 미국 경제와 미국 국민들에게 파괴적인 부담을 주었다. \\n\\n미국 상인들에 의한 바다와 내륙에서의 무역제한 회피, 법망의 허점은 유럽을 목표로 했던 엠바고의 영향을 급격히 감소시켰다. 영국 상인들은 엠바고 때문에 미국 배들에 의해 버려진 항로를 잘 이용해 먹었다 수입 금지 조치로 인해 촉발된 물자부족 현상으로 미국 남부에서 영국산 제품에 대한 수요가 치솟았다\n"
     ]
    }
   ],
   "source": [
    "valid_corpus = list(dict.fromkeys(example['context'] for example in dataset['validation']))[:10]\n",
    "sample_idx = random.choice(range(len(dataset['validation'])))\n",
    "query = dataset['validation'][sample_idx]['question']\n",
    "ground_truth = dataset['validation'][sample_idx]['context']\n",
    "print(query, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "350ff5cb-fc4e-40d2-aa04-1fc292f26ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ground_truth in valid_corpus:\n",
    "    valid_corpus.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8959452-aca0-490d-acee-b6b6ddda61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "    return tuple(t.cuda() for t in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46c2c93c-bc4b-4ee9-8a66-33268c141802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "    q_encoder.eval()\n",
    "    \n",
    "    q_seqs_val = tokenizer([query], padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "    q_emb = q_encoder(**q_seqs_val).to('cpu')\n",
    "    \n",
    "    p_embs = []\n",
    "    for p in valid_corpus:\n",
    "        p = tokenizer(p, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n",
    "        p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "        p_embs.append(p_emb)\n",
    "    \n",
    "    p_embs = torch.Tensor(p_embs).squeeze()\n",
    "    \n",
    "    print(p_embs.shape, q_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e96842a7-a37a-4ae6-9c07-ef05f9a3cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d80b5ff-273b-4c7f-b94b-424b43d7a733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[86.5999, 86.6622, 86.6466, 86.6187, 86.6514, 86.5946, 86.5708, 86.6117,\n",
      "         86.6401, 86.6131, 86.6357]])\n",
      "tensor([ 1,  4,  2,  8, 10,  3,  9,  7,  0,  5,  6])\n"
     ]
    }
   ],
   "source": [
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "print(dot_prod_scores)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd1159-5777-448d-be53-4a7ff1d8888f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

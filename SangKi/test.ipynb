{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'박상기'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "x = \"박상기 (朴商基)\"\n",
    "' '.join(re.sub(r'[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]', ' ', str(x.strip())).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "cd code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"./data/train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'미국 상의원 또는 미국 상원 United States Senate 은 양원제인 미국 의회의 상원이다 n n미국 부통령이 상원의장이 된다 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다 임기는 6년이며 2년마다 50개주 중 1 3씩 상원의원을 새로 선출하여 연방에 보낸다 n n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다 하원이 세금과 경제에 대한 권한 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다 즉 캘리포니아주 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다 그로 인하여 군대의 파병 관료의 임명에 대한 동의 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다 그리고 하원에 대한 견제 역할 하원의 법안을 거부할 권한 등 을 담당한다 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션 공공건강보험기관 의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다 날짜 2017 02 05'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x = dataset['train']['context'][0]\n",
    "' '.join(re.sub(r'[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]', ' ', str(x.strip())).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1745.0, style=ProgressStyle(description…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b5bc8548a0240fd9fc5b60fdcc38687"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=962.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39a8a2f0f4b147abb7a082de1fa537ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nDownloading and preparing dataset squad_kor_v1/squad_kor_v1 (download: 40.44 MiB, generated: 87.40 MiB, post-processed: Unknown size, total: 127.84 MiB) to /opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7568316.0, style=ProgressStyle(descript…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63e4c15cdf134f4cb0f1d25f6ba5dc31"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=770480.0, style=ProgressStyle(descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6352bce61a642d4b1ef35ce58f7c6d8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "822bc914be8343828bd09c761a7f569c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ""
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06358476b4ad44f5a902b008d36d531e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset squad_kor_v1 downloaded and prepared to /opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_kor_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd1368392959490f98ea4586f0eceea2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1da599a92bff4b51b06f1844391b104a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961828.0, style=ProgressStyle(descript…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1576eeff6d64a428fce3c45c55a2dc8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d226ed0dc7e449d915ce553eb634721"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 [UNK] 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡 ( 1악장 ) 을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tokenized_input = tokenizer(dataset['train'][0]['context'], padding=\"max_length\", truncation=True)\n",
    "tokenizer.decode(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Train with full dataset\n",
    "training_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aea10b0a646b43caa7b0c289b9c7342d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  p_encoder.cuda()\n",
    "  q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, dataset, p_model, q_model):\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  optimizer = AdamW([\n",
    "                {'params': p_model.parameters()},\n",
    "                {'params': q_model.parameters()}\n",
    "            ], lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2]\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5]}\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    "\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    "      print(loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "  return p_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Atensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14947/15102 [3:18:49<02:03,  1.25it/s]\u001b[Atensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14948/15102 [3:18:50<02:02,  1.26it/s]\u001b[Atensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14949/15102 [3:18:51<02:01,  1.26it/s]\u001b[Atensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14950/15102 [3:18:51<02:01,  1.26it/s]\u001b[Atensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14951/15102 [3:18:52<02:00,  1.26it/s]\u001b[Atensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14952/15102 [3:18:53<01:59,  1.26it/s]\u001b[Atensor(1.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14953/15102 [3:18:54<01:58,  1.26it/s]\u001b[Atensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14954/15102 [3:18:55<01:58,  1.25it/s]\u001b[Atensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14955/15102 [3:18:55<01:57,  1.25it/s]\u001b[Atensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14956/15102 [3:18:56<01:56,  1.26it/s]\u001b[Atensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14957/15102 [3:18:57<01:55,  1.25it/s]\u001b[Atensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14958/15102 [3:18:58<01:54,  1.25it/s]\u001b[Atensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14959/15102 [3:18:59<01:53,  1.25it/s]\u001b[Atensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14960/15102 [3:18:59<01:53,  1.25it/s]\u001b[Atensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14961/15102 [3:19:00<01:52,  1.26it/s]\u001b[Atensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14962/15102 [3:19:01<01:51,  1.25it/s]\u001b[Atensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14963/15102 [3:19:02<01:50,  1.25it/s]\u001b[Atensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14964/15102 [3:19:03<01:50,  1.25it/s]\u001b[Atensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14965/15102 [3:19:03<01:49,  1.25it/s]\u001b[Atensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14966/15102 [3:19:04<01:48,  1.25it/s]\u001b[Atensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14967/15102 [3:19:05<01:47,  1.25it/s]\u001b[Atensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14968/15102 [3:19:06<01:46,  1.25it/s]\u001b[Atensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14969/15102 [3:19:07<01:46,  1.25it/s]\u001b[Atensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14970/15102 [3:19:07<01:45,  1.25it/s]\u001b[Atensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14971/15102 [3:19:08<01:44,  1.25it/s]\u001b[Atensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14972/15102 [3:19:09<01:43,  1.25it/s]\u001b[Atensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14973/15102 [3:19:10<01:42,  1.25it/s]\u001b[Atensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14974/15102 [3:19:11<01:42,  1.25it/s]\u001b[Atensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14975/15102 [3:19:11<01:41,  1.25it/s]\u001b[Atensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14976/15102 [3:19:12<01:40,  1.25it/s]\u001b[Atensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14977/15102 [3:19:13<01:39,  1.25it/s]\u001b[Atensor(8.5376e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14978/15102 [3:19:14<01:38,  1.25it/s]\u001b[Atensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14979/15102 [3:19:14<01:38,  1.25it/s]\u001b[Atensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14980/15102 [3:19:15<01:37,  1.25it/s]\u001b[Atensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14981/15102 [3:19:16<01:36,  1.25it/s]\u001b[Atensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14982/15102 [3:19:17<01:35,  1.25it/s]\u001b[Atensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14983/15102 [3:19:18<01:34,  1.25it/s]\u001b[Atensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14984/15102 [3:19:18<01:34,  1.25it/s]\u001b[Atensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14985/15102 [3:19:19<01:33,  1.25it/s]\u001b[Atensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14986/15102 [3:19:20<01:32,  1.25it/s]\u001b[Atensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14987/15102 [3:19:21<01:31,  1.25it/s]\u001b[Atensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14988/15102 [3:19:22<01:31,  1.25it/s]\u001b[Atensor(0.4953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14989/15102 [3:19:22<01:30,  1.25it/s]\u001b[Atensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14990/15102 [3:19:23<01:29,  1.25it/s]\u001b[Atensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14991/15102 [3:19:24<01:28,  1.25it/s]\u001b[Atensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14992/15102 [3:19:25<01:27,  1.25it/s]\u001b[Atensor(1.8652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14993/15102 [3:19:26<01:27,  1.25it/s]\u001b[Atensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14994/15102 [3:19:26<01:26,  1.25it/s]\u001b[Atensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14995/15102 [3:19:27<01:25,  1.25it/s]\u001b[Atensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14996/15102 [3:19:28<01:24,  1.25it/s]\u001b[Atensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14997/15102 [3:19:29<01:23,  1.25it/s]\u001b[Atensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14998/15102 [3:19:30<01:23,  1.25it/s]\u001b[Atensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 14999/15102 [3:19:30<01:22,  1.25it/s]\u001b[Atensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15000/15102 [3:19:31<01:21,  1.25it/s]\u001b[Atensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15001/15102 [3:19:32<01:20,  1.25it/s]\u001b[Atensor(0.7689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15002/15102 [3:19:33<01:19,  1.25it/s]\u001b[Atensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15003/15102 [3:19:34<01:19,  1.25it/s]\u001b[Atensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15004/15102 [3:19:34<01:18,  1.25it/s]\u001b[Atensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15005/15102 [3:19:35<01:17,  1.25it/s]\u001b[Atensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15006/15102 [3:19:36<01:16,  1.25it/s]\u001b[Atensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15007/15102 [3:19:37<01:15,  1.25it/s]\u001b[Atensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15008/15102 [3:19:38<01:15,  1.25it/s]\u001b[Atensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15009/15102 [3:19:38<01:14,  1.25it/s]\u001b[Atensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15010/15102 [3:19:39<01:13,  1.25it/s]\u001b[Atensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15011/15102 [3:19:40<01:12,  1.25it/s]\u001b[Atensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15012/15102 [3:19:41<01:11,  1.25it/s]\u001b[Atensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15013/15102 [3:19:42<01:11,  1.25it/s]\u001b[Atensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15014/15102 [3:19:42<01:10,  1.25it/s]\u001b[Atensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15015/15102 [3:19:43<01:09,  1.25it/s]\u001b[Atensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15016/15102 [3:19:44<01:08,  1.25it/s]\u001b[Atensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15017/15102 [3:19:45<01:07,  1.25it/s]\u001b[Atensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15018/15102 [3:19:46<01:07,  1.25it/s]\u001b[Atensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15019/15102 [3:19:46<01:06,  1.25it/s]\u001b[Atensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15020/15102 [3:19:47<01:05,  1.25it/s]\u001b[Atensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15021/15102 [3:19:48<01:04,  1.25it/s]\u001b[Atensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15022/15102 [3:19:49<01:03,  1.25it/s]\u001b[Atensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15023/15102 [3:19:50<01:02,  1.25it/s]\u001b[Atensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15024/15102 [3:19:50<01:02,  1.25it/s]\u001b[Atensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15025/15102 [3:19:51<01:01,  1.25it/s]\u001b[Atensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration:  99%|█████████▉| 15026/15102 [3:19:52<01:00,  1.25it/s]\u001b[Atensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15027/15102 [3:19:53<00:59,  1.25it/s]\u001b[Atensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15028/15102 [3:19:54<00:58,  1.26it/s]\u001b[Atensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15029/15102 [3:19:54<00:58,  1.26it/s]\u001b[Atensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15030/15102 [3:19:55<00:57,  1.26it/s]\u001b[Atensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15031/15102 [3:19:56<00:56,  1.26it/s]\u001b[Atensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15032/15102 [3:19:57<00:55,  1.26it/s]\u001b[Atensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15033/15102 [3:19:58<00:54,  1.26it/s]\u001b[Atensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15034/15102 [3:19:58<00:54,  1.26it/s]\u001b[Atensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15035/15102 [3:19:59<00:53,  1.26it/s]\u001b[Atensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15036/15102 [3:20:00<00:52,  1.26it/s]\u001b[Atensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15037/15102 [3:20:01<00:51,  1.26it/s]\u001b[Atensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15038/15102 [3:20:02<00:50,  1.26it/s]\u001b[Atensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15039/15102 [3:20:02<00:50,  1.25it/s]\u001b[Atensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15040/15102 [3:20:03<00:49,  1.25it/s]\u001b[Atensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15041/15102 [3:20:04<00:48,  1.25it/s]\u001b[Atensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15042/15102 [3:20:05<00:47,  1.25it/s]\u001b[Atensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15043/15102 [3:20:06<00:47,  1.25it/s]\u001b[Atensor(0.9693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15044/15102 [3:20:06<00:46,  1.25it/s]\u001b[Atensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15045/15102 [3:20:07<00:45,  1.25it/s]\u001b[Atensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15046/15102 [3:20:08<00:44,  1.25it/s]\u001b[Atensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15047/15102 [3:20:09<00:43,  1.25it/s]\u001b[Atensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15048/15102 [3:20:10<00:43,  1.25it/s]\u001b[Atensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15049/15102 [3:20:10<00:42,  1.25it/s]\u001b[Atensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15050/15102 [3:20:11<00:41,  1.25it/s]\u001b[Atensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15051/15102 [3:20:12<00:40,  1.25it/s]\u001b[Atensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15052/15102 [3:20:13<00:39,  1.25it/s]\u001b[Atensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15053/15102 [3:20:14<00:39,  1.25it/s]\u001b[Atensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15054/15102 [3:20:14<00:38,  1.25it/s]\u001b[Atensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15055/15102 [3:20:15<00:37,  1.25it/s]\u001b[Atensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15056/15102 [3:20:16<00:36,  1.25it/s]\u001b[Atensor(1.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15057/15102 [3:20:17<00:35,  1.25it/s]\u001b[Atensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15058/15102 [3:20:18<00:35,  1.25it/s]\u001b[Atensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15059/15102 [3:20:18<00:34,  1.25it/s]\u001b[Atensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15060/15102 [3:20:19<00:33,  1.25it/s]\u001b[Atensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15061/15102 [3:20:20<00:32,  1.25it/s]\u001b[Atensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15062/15102 [3:20:21<00:31,  1.25it/s]\u001b[Atensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15063/15102 [3:20:22<00:31,  1.25it/s]\u001b[Atensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15064/15102 [3:20:22<00:30,  1.25it/s]\u001b[Atensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15065/15102 [3:20:23<00:29,  1.25it/s]\u001b[Atensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15066/15102 [3:20:24<00:28,  1.26it/s]\u001b[Atensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15067/15102 [3:20:25<00:27,  1.25it/s]\u001b[Atensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15068/15102 [3:20:25<00:27,  1.25it/s]\u001b[Atensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15069/15102 [3:20:26<00:26,  1.25it/s]\u001b[Atensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15070/15102 [3:20:27<00:25,  1.25it/s]\u001b[Atensor(0.3092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15071/15102 [3:20:28<00:24,  1.25it/s]\u001b[Atensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15072/15102 [3:20:29<00:23,  1.25it/s]\u001b[Atensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15073/15102 [3:20:29<00:23,  1.25it/s]\u001b[Atensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15074/15102 [3:20:30<00:22,  1.25it/s]\u001b[Atensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15075/15102 [3:20:31<00:21,  1.25it/s]\u001b[Atensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15076/15102 [3:20:32<00:20,  1.25it/s]\u001b[Atensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15077/15102 [3:20:33<00:19,  1.25it/s]\u001b[Atensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15078/15102 [3:20:33<00:19,  1.25it/s]\u001b[Atensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15079/15102 [3:20:34<00:18,  1.25it/s]\u001b[Atensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15080/15102 [3:20:35<00:17,  1.25it/s]\u001b[Atensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15081/15102 [3:20:36<00:16,  1.25it/s]\u001b[Atensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15082/15102 [3:20:37<00:15,  1.25it/s]\u001b[Atensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15083/15102 [3:20:37<00:15,  1.25it/s]\u001b[Atensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15084/15102 [3:20:38<00:14,  1.25it/s]\u001b[Atensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15085/15102 [3:20:39<00:13,  1.25it/s]\u001b[Atensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15086/15102 [3:20:40<00:12,  1.25it/s]\u001b[Atensor(2.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15087/15102 [3:20:41<00:11,  1.25it/s]\u001b[Atensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15088/15102 [3:20:41<00:11,  1.25it/s]\u001b[Atensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15089/15102 [3:20:42<00:10,  1.25it/s]\u001b[Atensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15090/15102 [3:20:43<00:09,  1.25it/s]\u001b[Atensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15091/15102 [3:20:44<00:08,  1.25it/s]\u001b[Atensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15092/15102 [3:20:45<00:07,  1.25it/s]\u001b[Atensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15093/15102 [3:20:45<00:07,  1.25it/s]\u001b[Atensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15094/15102 [3:20:46<00:06,  1.25it/s]\u001b[Atensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15095/15102 [3:20:47<00:05,  1.25it/s]\u001b[Atensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15096/15102 [3:20:48<00:04,  1.26it/s]\u001b[Atensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15097/15102 [3:20:49<00:03,  1.25it/s]\u001b[Atensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15098/15102 [3:20:49<00:03,  1.25it/s]\u001b[Atensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15099/15102 [3:20:50<00:02,  1.25it/s]\u001b[Atensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15100/15102 [3:20:51<00:01,  1.25it/s]\u001b[Atensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Iteration: 100%|█████████▉| 15101/15102 [3:20:52<00:00,  1.26it/s]\u001b[A"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (3) to match target batch_size (4).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-576e00cf89ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-206512b6423d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, dataset, p_model, q_model)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0msim_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 2216\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "p_encoder, q_encoder = train(args, train_dataset, p_encoder, q_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "문민정부 당시 역사바로세우기 정책으로 무엇을 시행하였는가?\n김영삼은 문민정부의 성격을 \"1993년 광주민중항쟁을 계승한 정부\"로 규정하고 광주민주화운동 등을 민주화 운동이라는 사실을 재확인하였으며 신군부 세력에게는 광주항쟁 유혈진압의 죄도 함께 물었다는 점이 있다. 또 집권 초 검은돈 거래를 차단하기 위해 1993년 8월 12일 모든 금융은 실명으로 거래하는 금융실명제를 도입하여 거래의 투명성을 가져왔다는 점도 높이 평가된다. 1995년부터는 일제 침략의 상징인 옛 조선총독부 청사를 폭파 철거하고 독도접안시설을 건설하였으며 일본의 역사 왜곡에 당당히 대항 못된 버르장머리를 고쳐 주겠다고 호언하는 등 일본의 야욕에 당당한 대통령이었다. 그리고 국민학교라는 명칭을 초등학교로 개칭하고 역사바로세우기의 일환으로 쇠말뚝뽑기로 민족의 기틀을 세웠다. \n\n\n"
     ]
    }
   ],
   "source": [
    "valid_corpus = list(set([example['context'] for example in dataset['validation']]))[:10]\n",
    "sample_idx = random.choice(range(len(dataset['validation'])))\n",
    "query = dataset['validation'][sample_idx]['question']\n",
    "ground_truth = dataset['validation'][sample_idx]['context']\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "  valid_corpus.append(ground_truth)\n",
    "\n",
    "print(query)\n",
    "print(ground_truth, '\\n\\n')\n",
    "\n",
    "# valid_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7ab210f85a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import copy\n",
    "​\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup, ElectraTokenizerFast, ElectraModel, ElectraPreTrainedModel\n",
    "from datasets import load_from_disk, load_dataset\n",
    "​\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "​\n",
    "# korquad dataset\n",
    "dataset = load_dataset(\"squad_kor_v1\")\n",
    "print(dataset)\n",
    "​\n",
    "# baseline\n",
    "model_checkpoint = 'bert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "​\n",
    "# custom\n",
    "# model_checkpoint = 'kykim/electra-kor-base'\n",
    "# tokenizer = ElectraTokenizerFast.from_pretrained(model_checkpoint)\n",
    "​\n",
    "training_dataset = dataset['train']\n",
    "q_seqs = tokenizer(training_dataset['question'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "                             q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "​\n",
    "validate_dataset = dataset['validation']\n",
    "q_seqs = tokenizer(validate_dataset['question'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(validate_dataset['context'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "valid_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'],\n",
    "                             q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "​\n",
    "class ElectraEncoder(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(ElectraEncoder, self).__init__(config)\n",
    "        \n",
    "        self.electra = ElectraModel(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output\n",
    "​\n",
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output\n",
    "​\n",
    "# baseline\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).cuda()\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).cuda()\n",
    "​\n",
    "# custom\n",
    "# p_encoder = ElectraEncoder.from_pretrained(model_checkpoint).cuda()\n",
    "# q_encoder = ElectraEncoder.from_pretrained(model_checkpoint).cuda()\n",
    "​\n",
    "args = TrainingArguments(\n",
    "    output_dir='dense_retrieval',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    "    )\n",
    "​\n",
    "def train(args, train_dataset, valid_dataset, p_model, q_model):\n",
    "    \n",
    "    # Dataloader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "    \n",
    "    # Dataloader\n",
    "    valid_sampler = RandomSampler(valid_dataset)\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.per_device_eval_batch_size)\n",
    "    \n",
    "    # Optimizer\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params' : [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay' : args.weight_decay},\n",
    "        {'params' : [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay' : 0.0},\n",
    "        {'params' : [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay' : args.weight_decay},\n",
    "        {'params' : [p for n, p in q_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay' : 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    t_total = len(train_loader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "    \n",
    "    # start training\n",
    "    global_step = 0\n",
    "    best_acc = 0\n",
    "    best_step = 0\n",
    "    \n",
    "    p_model.zero_grad()\n",
    "    q_model.zero_grad()\n",
    "    \n",
    "    for _ in range(args.num_train_epochs):\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            q_model.train()\n",
    "            p_model.train()\n",
    "            \n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "                \n",
    "            p_inputs = {'input_ids' : batch[0],\n",
    "                       'attention_mask' : batch[1],\n",
    "                       'token_type_ids' : batch[2]\n",
    "                       }\n",
    "            \n",
    "            q_inputs = {'input_ids' : batch[3],\n",
    "                       'attention_mask' : batch[4],\n",
    "                       'token_type_ids' : batch[5]\n",
    "                       }\n",
    "            \n",
    "            p_outputs = p_model(**p_inputs)\n",
    "            q_outputs = q_model(**q_inputs)\n",
    "            \n",
    "            sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))\n",
    "            sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "            targets = torch.arange(0, sim_scores.shape[0]).long().cuda()\n",
    "            \n",
    "            loss = F.nll_loss(sim_scores, targets)\n",
    "            train_losses.append(loss.item() / sim_scores.shape[0])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            q_model.zero_grad()\n",
    "            p_model.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % 1000 == 0:\n",
    "                valid_losses = []\n",
    "                valid_accs = []\n",
    "                with torch.no_grad():\n",
    "                    p_model.eval()\n",
    "                    q_model.eval()\n",
    "                    \n",
    "                    for batch in valid_loader:\n",
    "                        batch = tuple(t.cuda() for t in batch)\n",
    "                        \n",
    "                        p_inputs = {'input_ids' : batch[0],\n",
    "                                    'attention_mask' : batch[1],\n",
    "                                    'token_type_ids' : batch[2]\n",
    "                                   }\n",
    "            \n",
    "                        q_inputs = {'input_ids' : batch[3],\n",
    "                                    'attention_mask' : batch[4],\n",
    "                                    'token_type_ids' : batch[5]\n",
    "                                   }\n",
    "                \n",
    "                        p_outputs = p_model(**p_inputs)\n",
    "                        q_outputs = q_model(**q_inputs)\n",
    "                        \n",
    "                        sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))\n",
    "                        sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "                        targets = torch.arange(0, sim_scores.shape[0]).long().cuda()\n",
    "                        predict = torch.argmax(sim_scores, dim=1).long()\n",
    "                \n",
    "                        valid_loss = F.nll_loss(sim_scores, targets)      \n",
    "                        valid_acc = torch.mean((targets == predict).float())\n",
    "                        \n",
    "                        valid_losses.append(valid_loss.item() / sim_scores.shape[0])\n",
    "                        valid_accs.append(valid_acc.item())\n",
    "                        \n",
    "                valid_acc = np.mean(valid_accs)\n",
    "                print('train loss', np.mean(train_losses), 'valid loss', np.mean(valid_losses), 'valid acc', valid_acc)\n",
    "                if valid_acc > best_acc:\n",
    "                    best_acc = valid_acc\n",
    "                    best_step = global_step\n",
    "                    best_p_model = copy.deepcopy(p_model)\n",
    "                    best_q_model = copy.deepcopy(q_model)\n",
    "                    \n",
    "                if global_step > best_step + 5000:\n",
    "                    return best_p_model, best_q_model\n",
    "                    \n",
    "                train_losses = []\n",
    "                        \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return best_p_model, best_q_model\n",
    "​\n",
    "p_encoder, q_encoder = train(args, train_dataset, valid_dataset, p_encoder, q_encoder)\n",
    "torch.save(p_encoder.state_dict(), './p_encoder.pt')\n",
    "torch.save(q_encoder.state_dict(), './q_encoder.pt')"
   ]
  }
 ]
}